{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOTV2Gewagp0ngy0rbv8MpY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wfos3241/TextMining/blob/main/ex01_%ED%85%8D%EC%8A%A4%ED%8A%B8%EB%A7%88%EC%9D%B4%EB%8B%9D%EA%B8%B0%EC%B4%88.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "954YgKNbgEhl",
        "outputId": "c6963633-e47c-482d-da56-b831f2ad42b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "# nltk : 텍스트 관련 처리 라이브러리 (영문 중심)\n",
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "\n",
        "\n",
        "nltk.download('punkt_tab') # 토큰화 라이브러리\n",
        "nltk.download('wordnet') # 표제어 추출 라이브러리\n",
        "nltk.download('stopword') # 불용어 처리 라이브러리"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uuByynL1j6AO",
        "outputId": "d0f3a6bc-e6d7-42e7-f866-1a1a04b6f62e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Error loading stopword: Package 'stopword' not found in\n",
            "[nltk_data]     index\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 한글 자연어 처리 라이브러리\n",
        "!pip install konlpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1d_WnI6ikwRn",
        "outputId": "59f8436b-f63c-4b3d-980d-b874be094777"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting JPype1>=0.7.0 (from konlpy)\n",
            "  Downloading jpype1-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.11/dist-packages (from konlpy) (5.4.0)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.11/dist-packages (from konlpy) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from JPype1>=0.7.0->konlpy) (24.2)\n",
            "Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m85.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jpype1-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (494 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m494.1/494.1 kB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.5.2 konlpy-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NaQvv2VHmzjm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 텍스트마이닝 (Textmining)\n",
        "- 자연어 처리 (NLP : natural language processing) : 사람이 사용하는 일상의 말이나 언어의 의미를 분석해서 컴퓨터가 처리할 수 있도록 하는 작업\n",
        "\n",
        "# 자연어 (텍스트) 처리 과정\n",
        "- 전처리 : 오류수정, 결측치 처리 등\n",
        "- 토큰화 : 큰 문장을 작은 문장 또는 단어로 분리하는 작업\n",
        "- 인코딩 : AI 모델이 숫자를 기반으로 하기 때문에 문자로 된 데이터를 숫자 데이터로 변환하는 작업\n",
        "- 임베딩 : 단어간의 관계(상관성)을 분석해서 컴퓨터가 이해하도록 단어사전으로 만드는 작업"
      ],
      "metadata": {
        "id": "WmRsOG8im4Y-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 텍스트 전처리 작업의 종류\n",
        "- 오류수정 (정제 : cleaning)\n",
        "- 결측치 처리 : 일반적으로 삭제\n",
        "- 정규화 : 정규식을 사용해서 필요한 내용만 추출\n",
        "- 어간 추출 : 단어에서 중요한 부분만 추출 (일반적으로 앞쪽에 위치)\n",
        "  - (예) 만들었어요, 만들고, 만들다, 만듬 -> 만들다\n",
        "- 표제어 추출 : 대표단어를 추출\n",
        "  - (예) 돌아가셨어요, 죽었어요, 사망했어요 -> 죽었다\n",
        "- 불용어(stopword) : 학습에 사용하지 않을 단어\n",
        "- 텍스트 증식(증감)\n",
        "  - 텍스트 데이터가 적은 경우 데이터 수를 늘리는 작업\n",
        "  - 방법\n",
        "    - 단어 삭제 : 일부 단어를 삭제\n",
        "      - (예) 나는 학교에 갑니다 -> 학교에 갑니다\n",
        "    - 단어 교환\n",
        "      - (예) 나는 학교에 갑니다 -> 학교에 나는 갑니다\n",
        "    - 단어 추가\n",
        "      - (예) 나는 학교에 갑니다 -> 나는 버스로 학교에 갑니다\n",
        "    - 유사 단어로 변경\n",
        "      - (예) 나는 학교에 갑니다 -> 나는 교실에 갑니다\n",
        "    - 번역 후 재번역\n",
        "      - (예) 나는 학교에 갑니다 -> 我去上学 -> 나는 학교에 간다"
      ],
      "metadata": {
        "id": "SxEF8Oqro-Fu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 토큰화(Tokenization)\n",
        "- 문장(corpus)을 작은 문장(chunk)이나 단어(token)로 분리하는 작업\n",
        "- 영문 : 빈 공백으로 분리 (띄어쓰기가 잘 지켜지기 때문)\n",
        "- 한글 토큰화가 힘든 이유\n",
        "  - 띄어쓰기가 잘 지켜지지 않는다\n",
        "  - 꾸며주는 특성을 갖는 형용사/부사적 표현이 많음\n",
        "  - 의태어/의성어가 많음\n",
        "  - 형태소 분리기를 사용"
      ],
      "metadata": {
        "id": "v14Y_nrnt2rb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 영문 토큰화"
      ],
      "metadata": {
        "id": "mHEww3Jnywzh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text1 = \"I am actively looking for Ph.D. students. and you are a Ph.D. student.\"\n",
        "text2 = \"나는 버스를 타고 학교에 아침 일찍 갔습니다.\"\n",
        "text3 = \"나는 집에서 늦게 일어나 부랴부랴 준비하고 나왔더니 깜빡하고 전기장판을 안끄고 나왔습니다.\"\n",
        "print(word_tokenize(text1))\n",
        "print(word_tokenize(text2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TcuzHlVKnMC3",
        "outputId": "dc27cbbc-5dc5-435e-ec35-595e351af715"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'am', 'actively', 'looking', 'for', 'Ph.D.', 'students', '.', 'and', 'you', 'are', 'a', 'Ph.D.', 'student', '.']\n",
            "['나는', '버스를', '타고', '학교에', '아침', '일찍', '갔습니다', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 한글 토큰화\n",
        "  - 형태소 분리기 : Kkma, Hananum, Mecab, Twitter, Okt 등"
      ],
      "metadata": {
        "id": "mVRiiifOzV2Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import konlpy\n",
        "from konlpy.tag import Okt\n",
        "\n",
        "okt = Okt()\n",
        "\n",
        "# morphs() : 형태소 분리\n",
        "\n",
        "print(okt.morphs(text3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDFzd3ejxK36",
        "outputId": "54856303-b0cf-49c4-e0d8-aa890efee15e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['나', '는', '집', '에서', '늦게', '일어나', '부랴부랴', '준비', '하고', '나왔더니', '깜빡', '하고', '전기장판', '을', '안', '끄고', '나왔습니다', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(okt.pos(text3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZ2OxYASz9AY",
        "outputId": "072630e0-fb9a-4034-cd2e-84aaf2477dff"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('나', 'Noun'), ('는', 'Josa'), ('집', 'Noun'), ('에서', 'Josa'), ('늦게', 'Verb'), ('일어나', 'Verb'), ('부랴부랴', 'Noun'), ('준비', 'Noun'), ('하고', 'Josa'), ('나왔더니', 'Verb'), ('깜빡', 'Noun'), ('하고', 'Josa'), ('전기장판', 'Noun'), ('을', 'Josa'), ('안', 'VerbPrefix'), ('끄고', 'Verb'), ('나왔습니다', 'Verb'), ('.', 'Punctuation')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 명사만 추출\n",
        "\n",
        "print(okt.nouns(text2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iuhJKYDxz9YA",
        "outputId": "6226f414-a147-4172-b6f9-3dac04424c97"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['나', '버스', '타고', '학교', '아침', '일찍']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 정규화 : 필요한 데이터만 추출 - 정규식 사용"
      ],
      "metadata": {
        "id": "e4DxJxco1nA_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "\n",
        "corpus = \"소크라테스가 말했습니다. '너 자신을 알라 ^^*' ==> 무슨 의미일까요 ? now is 11 hour 35 minite\"\n",
        "\n",
        "# 정규식\n",
        "# ^ : not (아닌 것)\n",
        "# 가-힝 : 한글 전체 문자\n",
        "# ㄱ-ㅎ : 한글 자음\n",
        "# ㅏ-ㅣ : 한글 모음\n",
        "# A-Z : 영문 대문자\n",
        "# a-z : 영문 소문자\n",
        "# 0-9 : 숫자\n",
        "\n",
        "# 한글과 공백이 아닌 것\n",
        "\n",
        "result = re.compile(\"[^가-힝 ]\")\n",
        "\n",
        "# 한글과 공백이 아닌것을 삭제('') 하라\n",
        "\n",
        "print(result.sub('', corpus))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hq3mARQbz9zw",
        "outputId": "97e845a2-11d1-4004-ea41-8eb5f964ab7b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "소크라테스가 말했습니다 너 자신을 알라   무슨 의미일까요       \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 표제어 추출\n",
        "  - 대표단어 추출"
      ],
      "metadata": {
        "id": "_VyzHlor4Z9G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "word = ['leaves', 'lives', 'dies', 'has', 'children']\n",
        "\n",
        "print([lemmatizer.lemmatize(w) for w in word])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYZvsBoI1GAQ",
        "outputId": "ac7d2c08-3c62-4532-9097-dd7345a60111"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['leaf', 'life', 'dy', 'ha', 'child']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 어간추출"
      ],
      "metadata": {
        "id": "02coRPnJ5a-1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "s = PorterStemmer()\n",
        "\n",
        "print([s.stem(w) for w in word])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_zuMc-VR1GC4",
        "outputId": "c0883af0-b337-43a1-b2e0-67e4d4a35325"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['leav', 'live', 'die', 'ha', 'children']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 불용어 처리\n",
        "  - http://www.ranks.nl/stopwords/korean\n",
        "  - http://bab3min.tistory.com/544"
      ],
      "metadata": {
        "id": "PXpb4OKF6cht"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from konlpy.tag import Okt\n",
        "\n",
        "corpus = \"\"\"고기를 아무렇게나 구우려고 하면 안 돼. 고기라고 다 같은 게 아니거든.\n",
        "예컨데 삼겹살을 구울 때는 중요한 게 있지.\"\"\"\n",
        "\n",
        "# 불용어 목록\n",
        "\n",
        "stop_words = [\"를\", \"하면\", \"돼\", \".\", \"라고\", \"다\", \"게\", \"예컨데\", \"을\", \"는\"]\n",
        "\n",
        "# 형태소 분리\n",
        "\n",
        "okt = Okt()\n",
        "result = []\n",
        "\n",
        "# stem = 어간 추출 설정\n",
        "\n",
        "result = okt.morphs(corpus, stem = True)\n",
        "\n",
        "print(result)\n",
        "\n",
        "# 불용어 처리\n",
        "# for w in result : 형태소를 하나씩 읽어와서 w에 저장\n",
        "# if w not in stop_words : w값이 stop_word에 없다면\n",
        "# [w] : 리스트로 저장\n",
        "\n",
        "result = [w for w in result if w not in stop_words]\n",
        "\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v7yDGio91GFQ",
        "outputId": "a81068b3-92fc-4f40-fb78-f50b1399287e"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['고기', '를', '아무렇다', '구', '우려', '고', '하다', '안', '돼다', '.', '고기', '라고', '다', '같다', '게', '아니다', '.', '\\n', '예컨데', '삼겹살', '을', '구울', '때', '는', '중요하다', '게', '있다', '.']\n",
            "['고기', '아무렇다', '구', '우려', '고', '하다', '안', '돼다', '고기', '같다', '아니다', '\\n', '삼겹살', '구울', '때', '중요하다', '있다']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 인코딩\n",
        "- 학습을 위해 문자 데이터를 숫자 데이터로 변환하는 작업\n",
        "- Tokeninzer()\n",
        "- CountVectorizer()\n",
        "- TfldVectorizer()"
      ],
      "metadata": {
        "id": "FpFXihDNCJlC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Tokeninzer() 이용\n",
        "  - 빈도수 기반 분석\n",
        "  - 빈도수 순으로 내림차순 정렬\n",
        "    - 빈도수가 동일하면 사전순서로 정렬\n",
        "  - 정렬순으로 1부터 인덱스를 부여\n",
        "  - 문자 토큰을 해당 인덱스로 변환"
      ],
      "metadata": {
        "id": "ALHtWkIzDSvy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "\n",
        "# 인코딩에 적용되는 단어의 수 (인덱스 수)\n",
        "\n",
        "tokenizer = Tokenizer(num_words = 10)\n",
        "\n",
        "# 토큰의 빈도수를 분석\n",
        "\n",
        "tokenizer.fit_on_texts([result])\n",
        "print(tokenizer.word_index)\n",
        "\n",
        "# 인코딩 : 분석된 인덱스를 토큰에 적용\n",
        "\n",
        "en = tokenizer.texts_to_sequences([result])\n",
        "\n",
        "print(en)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kfFXlbuy1GH4",
        "outputId": "cb2a9017-2604-41a7-ef83-a213beb90eec"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'고기': 1, '아무렇다': 2, '구': 3, '우려': 4, '고': 5, '하다': 6, '안': 7, '돼다': 8, '같다': 9, '아니다': 10, '\\n': 11, '삼겹살': 12, '구울': 13, '때': 14, '중요하다': 15, '있다': 16}\n",
            "[[1, 2, 3, 4, 5, 6, 7, 8, 1, 9]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- padding\n",
        "  - 데이터의 길이를 동일하게 맞추는 작업"
      ],
      "metadata": {
        "id": "7oLLbJ4hGZpp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        " # 최종 데이터의 길이를 5로 설정\n",
        "\n",
        "result = pad_sequences(en, maxlen = 5, truncating = \"pre\")\n",
        "\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVlCXr_mD3F6",
        "outputId": "609dbdcf-36ee-4469-87c7-31dd86acd8ab"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[6 7 8 1 9]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aF2fCC1YD3IR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H10d0lmAD3KZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aFEmhSXTD3Ms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0FLA13PmD3O8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ymJWmS-G1GKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "68yra3dl1GNC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6EZDv0zhz92A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wgddX0J8z94c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FoxkkmENz9-A"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}