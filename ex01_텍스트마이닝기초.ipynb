{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPSm7KTCCrrFD6/0lT3MzkE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wfos3241/TextMining/blob/main/ex01_%ED%85%8D%EC%8A%A4%ED%8A%B8%EB%A7%88%EC%9D%B4%EB%8B%9D%EA%B8%B0%EC%B4%88.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import SGDRegressor\n",
        "from sklearn.linear_model import Lasso, Ridge\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error"
      ],
      "metadata": {
        "id": "8uv8jmBjqD6s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "954YgKNbgEhl",
        "outputId": "307ea12e-a55b-4615-b4c7-788b2f766621"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "# nltk : 텍스트 관련 처리 라이브러리 (영문 중심)\n",
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "\n",
        "\n",
        "nltk.download('punkt_tab') # 토큰화 라이브러리\n",
        "nltk.download('wordnet') # 표제어 추출 라이브러리\n",
        "nltk.download('stopword') # 불용어 처리 라이브러리"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uuByynL1j6AO",
        "outputId": "aae17638-c7f9-4e53-b150-31e393961a60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Error loading stopword: Package 'stopword' not found in\n",
            "[nltk_data]     index\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 한글 자연어 처리 라이브러리\n",
        "!pip install konlpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1d_WnI6ikwRn",
        "outputId": "07c0e964-f19d-48d8-cc2d-cf6734fc5668"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: konlpy in /usr/local/lib/python3.11/dist-packages (0.6.0)\n",
            "Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from konlpy) (1.5.2)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.11/dist-packages (from konlpy) (5.4.0)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.11/dist-packages (from konlpy) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from JPype1>=0.7.0->konlpy) (24.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 텍스트마이닝 (Textmining)\n",
        "- 자연어 처리 (NLP : natural language processing) : 사람이 사용하는 일상의 말이나 언어의 의미를 분석해서 컴퓨터가 처리할 수 있도록 하는 작업\n",
        "- 텍스트에서 컴퓨터가 이해할 수 있는 특성을 추출하는 작업\n",
        "\n",
        "- corpus (코퍼스) : 텍스트 전체 (큰 문장)\n",
        "- chunk (청크) : 작은 문장\n",
        "- token (토큰) : n개의 단어 (n-gram)\n",
        "- sub token (서브 토큰, 내부 단어) : 토큰을 한번 더 쪼개는 것\n",
        "  - 문자, 자소\n",
        "  - FastText : apple -> app, ppl, ple\n",
        "\n",
        "# 자연어 (텍스트) 처리 과정\n",
        "- 전처리 : 오류수정, 결측치 처리 등\n",
        "- 토큰화 : 큰 문장을 작은 문장 또는 단어로 분리하는 작업\n",
        "\n",
        "<center>  \n",
        "<img src=\"https://arome1004.cafe24.com/images/machine_learning/textmining02.png\" width=50%>\n",
        "</center>\n",
        "\n",
        "- 인코딩 : AI 모델이 숫자를 기반으로 하기 때문에 문자로 된 데이터를 숫자 데이터로 변환하는 작업\n",
        "\n",
        "<center>  \n",
        "<img src=\"https://arome1004.cafe24.com/images/machine_learning/textmining03.png\" width=50%>\n",
        "</center>\n",
        "  \n",
        "- padding : 데이터의 길이를 동일하게 맞춰주는 작업\n",
        "\n",
        "<center>  \n",
        "<img src=\"https://arome1004.cafe24.com/images/machine_learning/textmining04.png\" width=50%>\n",
        "</center>\n",
        "\n",
        "- 임베딩 : 단어간의 관계(상관성)을 분석해서 컴퓨터가 이해하도록 단어사전으로 만드는 작업\n",
        "\n",
        "<center>  \n",
        "<img src=\"https://arome1004.cafe24.com/images/machine_learning/textmining05.png\" width=50%>\n",
        "</center>"
      ],
      "metadata": {
        "id": "WmRsOG8im4Y-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center>  \n",
        "<img src=\"https://arome1004.cafe24.com/images/machine_learning/textmining01.png\" width=50%>\n",
        "</center>"
      ],
      "metadata": {
        "id": "Iu1XrLCKDfjX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 텍스트 전처리 작업의 종류\n",
        "- 오류수정 (정제 : cleaning)\n",
        "- 결측치 처리 : 일반적으로 삭제\n",
        "- 정규화 : 정규식을 사용해서 필요한 내용만 추출\n",
        "- 어간 추출 : 단어에서 중요한 부분만 추출 (일반적으로 앞쪽에 위치)\n",
        "  - (예) 만들었어요, 만들고, 만들다, 만듬 -> 만들다\n",
        "- 표제어 추출 : 대표단어를 추출\n",
        "  - (예) 돌아가셨어요, 죽었어요, 사망했어요 -> 죽었다\n",
        "- 불용어(stopword) : 학습에 사용하지 않을 단어\n",
        "- 텍스트 증식(증감)\n",
        "  - 텍스트 데이터가 적은 경우 데이터 수를 늘리는 작업\n",
        "  - 방법\n",
        "    - 단어 삭제 : 일부 단어를 삭제\n",
        "      - (예) 나는 학교에 갑니다 -> 학교에 갑니다\n",
        "    - 단어 교환\n",
        "      - (예) 나는 학교에 갑니다 -> 학교에 나는 갑니다\n",
        "    - 단어 추가\n",
        "      - (예) 나는 학교에 갑니다 -> 나는 버스로 학교에 갑니다\n",
        "    - 유사 단어로 변경\n",
        "      - (예) 나는 학교에 갑니다 -> 나는 교실에 갑니다\n",
        "    - 번역 후 재번역\n",
        "      - (예) 나는 학교에 갑니다 -> 我去上学 -> 나는 학교에 간다"
      ],
      "metadata": {
        "id": "SxEF8Oqro-Fu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 토큰화(Tokenization)\n",
        "- 문장(corpus)을 작은 문장(chunk)이나 단어(token)로 분리하는 작업\n",
        "- 영문 : 빈 공백으로 분리 (띄어쓰기가 잘 지켜지기 때문)\n",
        "- 한글 토큰화가 힘든 이유\n",
        "  - 띄어쓰기가 잘 지켜지지 않는다\n",
        "  - 꾸며주는 특성을 갖는 형용사/부사적 표현이 많음\n",
        "  - 의태어/의성어가 많음\n",
        "  - 형태소 분리기를 사용"
      ],
      "metadata": {
        "id": "v14Y_nrnt2rb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 영문 토큰화"
      ],
      "metadata": {
        "id": "mHEww3Jnywzh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text1 = \"I am actively looking for Ph.D. students. and you are a Ph.D. student.\"\n",
        "text2 = \"나는 버스를 타고 학교에 아침 일찍 갔습니다.\"\n",
        "text3 = \"나는 집에서 늦게 일어나 부랴부랴 준비하고 나왔더니 깜빡하고 전기장판을 안끄고 나왔습니다.\"\n",
        "print(word_tokenize(text1))\n",
        "print(word_tokenize(text2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TcuzHlVKnMC3",
        "outputId": "c3e0a711-a088-4b48-852a-0791dc30b60d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'am', 'actively', 'looking', 'for', 'Ph.D.', 'students', '.', 'and', 'you', 'are', 'a', 'Ph.D.', 'student', '.']\n",
            "['나는', '버스를', '타고', '학교에', '아침', '일찍', '갔습니다', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 한글 토큰화\n",
        "  - 형태소 분리기 : Kkma, Hananum, Mecab, Twitter, Okt 등"
      ],
      "metadata": {
        "id": "mVRiiifOzV2Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import konlpy\n",
        "from konlpy.tag import Okt\n",
        "\n",
        "okt = Okt()\n",
        "\n",
        "# morphs() : 형태소 분리\n",
        "\n",
        "print(okt.morphs(text3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDFzd3ejxK36",
        "outputId": "97d27638-6f1d-434b-fe23-42be3a23411e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['나', '는', '집', '에서', '늦게', '일어나', '부랴부랴', '준비', '하고', '나왔더니', '깜빡', '하고', '전기장판', '을', '안', '끄고', '나왔습니다', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(okt.pos(text3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZ2OxYASz9AY",
        "outputId": "523b8123-4f99-49ea-900a-7dfb4b3a83c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('나', 'Noun'), ('는', 'Josa'), ('집', 'Noun'), ('에서', 'Josa'), ('늦게', 'Verb'), ('일어나', 'Verb'), ('부랴부랴', 'Noun'), ('준비', 'Noun'), ('하고', 'Josa'), ('나왔더니', 'Verb'), ('깜빡', 'Noun'), ('하고', 'Josa'), ('전기장판', 'Noun'), ('을', 'Josa'), ('안', 'VerbPrefix'), ('끄고', 'Verb'), ('나왔습니다', 'Verb'), ('.', 'Punctuation')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 명사만 추출\n",
        "\n",
        "print(okt.nouns(text2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iuhJKYDxz9YA",
        "outputId": "a9405e08-505c-4617-fbd7-b02c7c70153b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['나', '버스', '타고', '학교', '아침', '일찍']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 정규화 : 필요한 데이터만 추출 - 정규식 사용"
      ],
      "metadata": {
        "id": "e4DxJxco1nA_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 정규 표현식 (Regular Expression)\n",
        "\n",
        "<table>\n",
        "<thead>\n",
        "<tr>\n",
        "<th>특수 문자</th>\n",
        "<th>설명</th>\n",
        "</tr>\n",
        "</thead>\n",
        "<tbody>\n",
        "<tr>\n",
        "<td>.</td>\n",
        "<td>한 개의 임의의 문자를 나타냄. (줄바꿈 문자인 \\n는 제외)</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>?</td>\n",
        "<td>앞의 문자가 존재할 수도 있고, 존재하지 않을 수도 있음. (문자가 0개 또는 1개)</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>*</td>\n",
        "<td>앞의 문자가 무한개로 존재할 수도 있고, 존재하지 않을 수도 있음. (문자가 0개 이상)</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>+</td>\n",
        "<td>앞의 문자가 최소 한 개 이상 존재. (문자가 1개 이상)</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>^</td>\n",
        "<td>뒤의 문자로 문자열이 시작.</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>$</td>\n",
        "<td>앞의 문자로 문자열이 종료.</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>{숫자}</td>\n",
        "<td>숫자만큼 반복.</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>{숫자1, 숫자2}</td>\n",
        "<td>숫자1 이상 숫자2 이하만큼 반복합니다. ?, *, +를 이것으로 대체.</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>{숫자,}</td>\n",
        "<td>숫자 이상만큼 반복합니다.</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>[ ]</td>\n",
        "<td>대괄호 안의 문자들 중 한 개의 문자와 매치. [amk]라고 한다면 a 또는 m 또는 k 중 하나라도 존재하면 매치를 의미. [a-zA-Z]는 알파벳 전체를 의미</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>[^문자]</td>\n",
        "<td>해당 문자를 제외한 문자를 매치.</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>l</td>\n",
        "<td>AlB와 같이 쓰이며 A 또는 B의 의미</td>\n",
        "</tr>\n",
        "</tbody>\n",
        "</table>\n",
        "\n",
        "\n",
        "<h3 id=\"2\"><strong>역 슬래쉬(\\)를 이용하여 자주 쓰이는 문자 규칙</strong></h3>\n",
        "\n",
        "<table>\n",
        "<thead>\n",
        "<tr>\n",
        "<th>문자 규칙</th>\n",
        "<th>설명</th>\n",
        "</tr>\n",
        "</thead>\n",
        "<tbody>\n",
        "<tr>\n",
        "<td>\\\\</td>\n",
        "<td>역 슬래쉬 문자 자체를 의미</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>\\d</td>\n",
        "<td>모든 숫자를 의미합니다. [0-9]와 의미가 동일.</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>\\D</td>\n",
        "<td>숫자를 제외한 모든 문자를 의미합니다. [^0-9]와 의미가 동일.</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>\\s</td>\n",
        "<td>공백을 의미합니다. [ \\t\\n\\r\\f\\v]와 의미가 동일.</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>\\S</td>\n",
        "<td>공백을 제외한 문자를 의미합니다. [^ \\t\\n\\r\\f\\v]와 의미가 동일.</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>\\w</td>\n",
        "<td>문자 또는 숫자를 의미합니다. [a-zA-Z0-9]와 의미가 동일.</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>\\W</td>\n",
        "<td>문자 또는 숫자가 아닌 문자를 의미합니다. [^a-zA-Z0-9]와 의미가 동일.</td>\n",
        "</tr>\n",
        "</tbody>\n",
        "</table>\n",
        "\n",
        "<h3 ><strong>정규표현식 모듈 함수</strong></h3>\n",
        "\n",
        "<table>\n",
        "<thead>\n",
        "<tr>\n",
        "<th>모듈 함수</th>\n",
        "<th>설명</th>\n",
        "</tr>\n",
        "</thead>\n",
        "<tbody>\n",
        "<tr>\n",
        "<td>re.compile()</td>\n",
        "<td>정규표현식을 컴파일하는 함수, 미리 컴파일해놓고 사용하면 속도와 편의성면에서 유리</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>re.search()</td>\n",
        "<td>문자열 전체에 대해서 정규표현식과 매치되는지를 검색.</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>re.match()</td>\n",
        "<td>문자열의 처음이 정규표현식과 매치되는지를 검색.</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>re.split()</td>\n",
        "<td>정규 표현식을 기준으로 문자열을 분리하여 리스트로 리턴.</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>re.findall()</td>\n",
        "<td>문자열에서 정규 표현식과 매치되는 모든 경우의 문자열을 찾아서 리스트로 리턴. 만약, 매치되는 문자열이 없다면 빈 리스트가 리턴.</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>re.finditer()</td>\n",
        "<td>문자열에서 정규 표현식과 매치되는 모든 경우의 문자열에 대한 이터레이터 객체를 리턴.</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>re.sub()</td>\n",
        "<td>문자열에서 정규 표현식과 일치하는 부분에 대해서 다른 문자열로 대체.</td>\n",
        "</tr>\n",
        "</tbody>\n",
        "</table>"
      ],
      "metadata": {
        "id": "y830iclkLHR3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "\n",
        "corpus = \"소크라테스가 말했습니다. '너 자신을 알라 ^^*' ==> 무슨 의미일까요 ? now is 11 hour 35 minite\"\n",
        "\n",
        "# 정규식\n",
        "# ^ : not (아닌 것)\n",
        "# 가-힝 : 한글 전체 문자\n",
        "# ㄱ-ㅎ : 한글 자음\n",
        "# ㅏ-ㅣ : 한글 모음\n",
        "# A-Z : 영문 대문자\n",
        "# a-z : 영문 소문자\n",
        "# 0-9 : 숫자\n",
        "\n",
        "# 한글과 공백이 아닌 것\n",
        "\n",
        "result = re.compile(\"[^가-힝 ]\")\n",
        "\n",
        "# 한글과 공백이 아닌것을 삭제('') 하라\n",
        "\n",
        "print(result.sub('', corpus))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hq3mARQbz9zw",
        "outputId": "a92e24cc-702c-4353-e60a-18d9cc60e32b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "소크라테스가 말했습니다 너 자신을 알라   무슨 의미일까요       \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "text = \"\"\"100 Jone        PROF\n",
        "          101 James       STUD\n",
        "          102 Max         STUD\"\"\"\n",
        "\n",
        "# 토큰화\n",
        "# 최소 공백이 1개 이상으로 된 것을 중심으로 분리\n",
        "\n",
        "re.split(\"\\s+\", text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pfpLyuGhMgiX",
        "outputId": "5eeed40f-a2d3-4431-cb9e-1aa4bb5ec4be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['100', 'Jone', 'PROF', '101', 'James', 'STUD', '102', 'Max', 'STUD']"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 숫자만 추출\n",
        "\n",
        "print(re.findall(\"[0-9]+\", text))\n",
        "print(re.findall(\"\\d+\", text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1i0YyxbMMgeC",
        "outputId": "5caa5df9-a48e-4bed-f20c-1400a280d0b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['100', '101', '102']\n",
            "['100', '101', '102']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 이름만 추출\n",
        "\n",
        "re.findall(\"[A-Z][a-z]+\", text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zRjEcEt2MgbJ",
        "outputId": "6ac9e842-fc1e-4bf7-bc1d-89963a39dec4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Jone', 'James', 'Max']"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 전화번호만 추출\n",
        "\n",
        "str1 = \"\"\"홍길동 062-334-3434\n",
        "          성춘향 02-455-3432\n",
        "          이순신 070-3534-5344\n",
        "          돌쇠   010-3232-4533\n",
        "          마당쇠 019-334-4544\n",
        "          김유신 34-32342-434533\"\"\"\n",
        "\n",
        "re.findall(\"\\d{2,3}-\\d{3,4}-\\d{4}\", str1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kF8v7hL-MgYX",
        "outputId": "9f3cab14-00bc-4d09-e05d-dd329ee80b05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['062-334-3434',\n",
              " '02-455-3432',\n",
              " '070-3534-5344',\n",
              " '010-3232-4533',\n",
              " '019-334-4544']"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 핸드폰 번호만 추출\n",
        "\n",
        "re.findall(\"01[016789]-\\d{3,4}-\\d{4}\", str1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_ixtXwRTUMg",
        "outputId": "63d78680-5b92-459d-907d-a4041d6dec63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['010-3232-4533', '019-334-4544']"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- URL 주소 추출"
      ],
      "metadata": {
        "id": "-NzPjm-6YNt_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"사이트 주소\n",
        "          (1) https://example.com\n",
        "          (2) http://www.naver.com, https://m.naver.co.kr\n",
        "          (3) www.daum.co.kr\n",
        "          (4) ttp://apt.com\n",
        "\"\"\"\n",
        "\n",
        "re.findall(\"https?://[^\\s,]+|www\\.[^\\s,]+\", text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FbLlIuNKTUWP",
        "outputId": "a1676a14-7df6-4431-b841-f0dcb9636684"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['https://example.com',\n",
              " 'http://www.naver.com',\n",
              " 'https://m.naver.co.kr',\n",
              " 'www.daum.co.kr']"
            ]
          },
          "metadata": {},
          "execution_count": 164
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 이메일 패턴"
      ],
      "metadata": {
        "id": "SVWMqvQrcVvn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"이메일 주소\n",
        "    - aaaa@gmail.com\n",
        "    - bk234@hanmail.net\n",
        "    - q14ds@naver.co.kr\n",
        "    - a_ddd@nK_3.N234.co.kr\n",
        "    - a@1.2.2\n",
        "    - 1@1.2\n",
        "\"\"\"\n",
        "\n",
        "re.findall(\"[A-Za-z0-9][A-Za-z0-9._-]*@[A-Za-z0-9][A-Za-z0-9._-]*\\.[a-zA-Z]{2,}+\", text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_ip4hWITUcw",
        "outputId": "02c5d8d8-7aae-4773-86a9-977a5576bfe6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['aaaa@gmail.com',\n",
              " 'bk234@hanmail.net',\n",
              " 'q14ds@naver.co.kr',\n",
              " 'a_ddd@nK_3.N234.co.kr']"
            ]
          },
          "metadata": {},
          "execution_count": 216
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 표제어 추출\n",
        "  - 대표단어 추출\n",
        "\n",
        "<center>  \n",
        "<img src=\"https://arome1004.cafe24.com/images/machine_learning/textmining06.png\" width=70%>\n",
        "</center>"
      ],
      "metadata": {
        "id": "_VyzHlor4Z9G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "word = ['leaves', 'lives', 'dies', 'has', 'children']\n",
        "\n",
        "print([lemmatizer.lemmatize(w) for w in word])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYZvsBoI1GAQ",
        "outputId": "e6202d0c-af9e-49aa-9f0d-2580f79080fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['leaf', 'life', 'dy', 'ha', 'child']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 어간추출"
      ],
      "metadata": {
        "id": "02coRPnJ5a-1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "s = PorterStemmer()\n",
        "\n",
        "print([s.stem(w) for w in word])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_zuMc-VR1GC4",
        "outputId": "e1196070-9742-4eaf-e57e-6eff9610fbf0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['leav', 'live', 'die', 'ha', 'children']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 불용어 처리\n",
        "  - http://www.ranks.nl/stopwords/korean\n",
        "  - http://bab3min.tistory.com/544"
      ],
      "metadata": {
        "id": "PXpb4OKF6cht"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from konlpy.tag import Okt\n",
        "\n",
        "corpus = \"\"\"고기를 아무렇게나 구우려고 하면 안 돼. 고기라고 다 같은 게 아니거든.\n",
        "예컨데 삼겹살을 구울 때는 중요한 게 있지.\"\"\"\n",
        "\n",
        "# 불용어 목록\n",
        "\n",
        "stop_words = [\"를\", \"하면\", \"돼\", \".\", \"라고\", \"다\", \"게\", \"예컨데\", \"을\", \"는\"]\n",
        "\n",
        "# 형태소 분리\n",
        "\n",
        "okt = Okt()\n",
        "result = []\n",
        "\n",
        "# stem = 어간 추출 설정\n",
        "\n",
        "result = okt.morphs(corpus, stem = True)\n",
        "\n",
        "print(result)\n",
        "\n",
        "# 불용어 처리\n",
        "# for w in result : 형태소를 하나씩 읽어와서 w에 저장\n",
        "# if w not in stop_words : w값이 stop_word에 없다면\n",
        "# [w] : 리스트로 저장\n",
        "\n",
        "result = [w for w in result if w not in stop_words]\n",
        "\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v7yDGio91GFQ",
        "outputId": "074b01e5-a2c5-4054-ac03-5e1ed445a877"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['고기', '를', '아무렇다', '구', '우려', '고', '하다', '안', '돼다', '.', '고기', '라고', '다', '같다', '게', '아니다', '.', '\\n', '예컨데', '삼겹살', '을', '구울', '때', '는', '중요하다', '게', '있다', '.']\n",
            "['고기', '아무렇다', '구', '우려', '고', '하다', '안', '돼다', '고기', '같다', '아니다', '\\n', '삼겹살', '구울', '때', '중요하다', '있다']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 인코딩\n",
        "- 학습을 위해 문자 데이터를 숫자 데이터로 변환하는 작업\n",
        "- Tokeninzer()\n",
        "- CountVectorizer()\n",
        "- TfldVectorizer()"
      ],
      "metadata": {
        "id": "FpFXihDNCJlC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Tokeninzer() 이용\n",
        "  - 빈도수 기반 분석\n",
        "  - 빈도수 순으로 내림차순 정렬\n",
        "    - 빈도수가 동일하면 단어의 등장 순서대로 정렬\n",
        "  - 정렬순으로 1부터 인덱스를 부여\n",
        "  - 문자 토큰을 해당 인덱스로 변환"
      ],
      "metadata": {
        "id": "ALHtWkIzDSvy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "\n",
        "# 인코딩에 적용되는 단어의 수 (인덱스 수)\n",
        "\n",
        "tokenizer = Tokenizer(num_words = 10)\n",
        "\n",
        "# 토큰의 빈도수를 분석\n",
        "\n",
        "tokenizer.fit_on_texts([result])\n",
        "print(tokenizer.word_index)\n",
        "\n",
        "# 인코딩 : 분석된 인덱스를 토큰에 적용\n",
        "\n",
        "en = tokenizer.texts_to_sequences([result])\n",
        "\n",
        "print(en)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kfFXlbuy1GH4",
        "outputId": "54bf827a-5e33-476c-c98e-8350fc78a42b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'고기': 1, '아무렇다': 2, '구': 3, '우려': 4, '고': 5, '하다': 6, '안': 7, '돼다': 8, '같다': 9, '아니다': 10, '\\n': 11, '삼겹살': 12, '구울': 13, '때': 14, '중요하다': 15, '있다': 16}\n",
            "[[1, 2, 3, 4, 5, 6, 7, 8, 1, 9]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- padding\n",
        "  - 데이터의 길이를 동일하게 맞추는 작업"
      ],
      "metadata": {
        "id": "7oLLbJ4hGZpp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        " # 최종 데이터의 길이를 20으로 설정\n",
        " # 길이가 maxlen보다 길면 왼쪽 부분의 나머지는 버림\n",
        " # 길이가 maxlen보다 짧으면 왼쪽에 0을 추가해서 맞춤\n",
        "\n",
        "result = pad_sequences(en, maxlen = 20, truncating = \"pre\")\n",
        "\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVlCXr_mD3F6",
        "outputId": "cdf87a78-da0e-470d-9d2b-3f74f1feac28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 0 0 0 0 0 0 0 0 0 1 2 3 4 5 6 7 8 1 9]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- BoW (Bag of Word)\n",
        " - 단어의 순서를 고려하지 않고 빈도수만 이용해서 텍스트를 수치화하는 방법\n",
        " - CountVectorizer()\n",
        " - TildfVectorizer()\n",
        "\n",
        "<center>  \n",
        "<img src=\"https://arome1004.cafe24.com/images/machine_learning/textmining07.png\" width=50%><br>\n",
        "<font size=1>참고 : https://medium.com/@vamshiprakash001/an-introduction-to-bag-of-words-bow-c32a65293ccc</font>\n",
        "</center>"
      ],
      "metadata": {
        "id": "CgNXmu7Ng5zQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- CountVectorizer()\n",
        "  - 토큰을 사전순으로 오름차순 정렬\n",
        "  - 정렬된 순으로 인덱스를 0부터 부여\n",
        "  - 각 토큰별로 빈도수를 분석\n",
        "  - 각 토큰별를 빈도수의 수치로 할당\n",
        "  - 토큰의 수가 많아지면 크기가 커짐 (원핫인코딩)\n",
        "    - 같은 길이로 만듬 -> padding이 필요 없음"
      ],
      "metadata": {
        "id": "drqF_k1_hR-I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from konlpy.tag import Okt\n",
        "\n",
        "okt = Okt()\n",
        "\n",
        "cv = CountVectorizer()\n",
        "cv_fit = cv.fit_transform([corpus])\n",
        "\n",
        "print(\"인덱스 부여 결과 : \", cv.vocabulary_)\n",
        "print(\"인코딩 결과 : \", cv_fit.toarray())"
      ],
      "metadata": {
        "id": "aF2fCC1YD3IR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24b2b875-9c4b-4209-8066-71f4a16aab8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "인덱스 부여 결과 :  {'고기를': 2, '아무렇게나': 8, '구우려고': 3, '하면': 12, '고기라고': 1, '같은': 0, '아니거든': 7, '예컨데': 9, '삼겹살을': 6, '구울': 4, '때는': 5, '중요한': 11, '있지': 10}\n",
            "인코딩 결과 :  [[1 1 1 1 1 1 1 1 1 1 1 1 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- TildfVectorizer()\n",
        "  - TF(Text Frequency) : 토큰의 빈도수\n",
        "  - DF(Document Frequency) : 토큰이 등장하는 문장의 수\n",
        "  - IDF(Inverse DF) : 문장의 자주 등장하는 토큰은 문장을 구분하는데 중요하지 않다 (반비례)\n",
        "  - N : 확률을 구하기 위한 것\n",
        "  - log : 토큰이 문장에 들어가는 분포를 보면 많이 들어가는 토큰과 그렇지 못한 토큰의 비율이 지수 특성을 갖음 -> 선형으로 변환 -> 학습 성능이 올라감\n",
        "\n",
        "<center>  \n",
        "<img src=\"https://arome1004.cafe24.com/images/machine_learning/tf_idf.png\" width=50%>\n",
        "</center>   "
      ],
      "metadata": {
        "id": "Y8IlIipPjkrf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tv = TfidfVectorizer()\n",
        "tv_fit = tv.fit_transform([corpus])\n",
        "\n",
        "print(\"인덱스 부여 결과 : \", tv.vocabulary_)\n",
        "print(\"인코딩 결과 : \", tv_fit.toarray())"
      ],
      "metadata": {
        "id": "H10d0lmAD3KZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a63c926-24bc-4731-8517-5c85f0f6324f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "인덱스 부여 결과 :  {'고기를': 2, '아무렇게나': 8, '구우려고': 3, '하면': 12, '고기라고': 1, '같은': 0, '아니거든': 7, '예컨데': 9, '삼겹살을': 6, '구울': 4, '때는': 5, '중요한': 11, '있지': 10}\n",
            "인코딩 결과 :  [[0.2773501 0.2773501 0.2773501 0.2773501 0.2773501 0.2773501 0.2773501\n",
            "  0.2773501 0.2773501 0.2773501 0.2773501 0.2773501 0.2773501]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 텍스트 마이닝 실습"
      ],
      "metadata": {
        "id": "ziCD7Od-neot"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [\"제품을 잘 쓰고 쓰고 있어요\",\n",
        "          \"제품의 디자인이 우수해요\",\n",
        "          \"성능이 우수해요\",\n",
        "          \"제품에 손상이 있어요\",\n",
        "          \"제품 가격이 비싸요\",\n",
        "          \"정말 필요한 제품입니다\",\n",
        "          \"가격이 비싸고 서비스도 엉망이예요\",\n",
        "          \"가격대비 성능이 우수해요\",\n",
        "          \"디자인이 좋아요\",\n",
        "          \"필요없는 기능이 많은듯 해요\",\n",
        "          \"서비스 대응이 나빠요\",\n",
        "          \"제품에 손상이 심해요\"]\n",
        "labels = [1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0]"
      ],
      "metadata": {
        "id": "aFEmhSXTD3Ms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(num_words = 100)\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "en = tokenizer.texts_to_sequences(corpus)\n",
        "print(tokenizer.word_index)\n",
        "print(en)\n",
        "rs = pad_sequences(en, maxlen = 10, truncating = \"pre\")\n",
        "print(rs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3mUqGN0pyKc",
        "outputId": "0a87f9aa-fb42-40a8-b72b-bd99e1bbf30e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'우수해요': 1, '쓰고': 2, '있어요': 3, '디자인이': 4, '성능이': 5, '제품에': 6, '손상이': 7, '가격이': 8, '제품을': 9, '잘': 10, '제품의': 11, '제품': 12, '비싸요': 13, '정말': 14, '필요한': 15, '제품입니다': 16, '비싸고': 17, '서비스도': 18, '엉망이예요': 19, '가격대비': 20, '좋아요': 21, '필요없는': 22, '기능이': 23, '많은듯': 24, '해요': 25, '서비스': 26, '대응이': 27, '나빠요': 28, '심해요': 29}\n",
            "[[9, 10, 2, 2, 3], [11, 4, 1], [5, 1], [6, 7, 3], [12, 8, 13], [14, 15, 16], [8, 17, 18, 19], [20, 5, 1], [4, 21], [22, 23, 24, 25], [26, 27, 28], [6, 7, 29]]\n",
            "[[ 0  0  0  0  0  9 10  2  2  3]\n",
            " [ 0  0  0  0  0  0  0 11  4  1]\n",
            " [ 0  0  0  0  0  0  0  0  5  1]\n",
            " [ 0  0  0  0  0  0  0  6  7  3]\n",
            " [ 0  0  0  0  0  0  0 12  8 13]\n",
            " [ 0  0  0  0  0  0  0 14 15 16]\n",
            " [ 0  0  0  0  0  0  8 17 18 19]\n",
            " [ 0  0  0  0  0  0  0 20  5  1]\n",
            " [ 0  0  0  0  0  0  0  0  4 21]\n",
            " [ 0  0  0  0  0  0 22 23 24 25]\n",
            " [ 0  0  0  0  0  0  0 26 27 28]\n",
            " [ 0  0  0  0  0  0  0  6  7 29]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(rs, labels, test_size = 0.3, random_state = 50)\n",
        "\n",
        "lr_model = LinearRegression()\n",
        "sgd_model = SGDRegressor(eta0 = 0.01, max_iter = 5000, verbose = 1)"
      ],
      "metadata": {
        "id": "jHXl8bwcp_TU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr_model.fit(X_train, y_train)\n",
        "sgd_model.fit(X_train, y_train)\n",
        "pre = lr_model.predict(X_test)\n",
        "pre2 = sgd_model.predict(X_test)\n",
        "print(lr_model.score(X_test, y_test))\n",
        "print(mean_squared_error(y_test, pre))\n",
        "print(sgd_model.score(X_test, y_test))\n",
        "print(mean_squared_error(y_test, pre2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hlvVJo93sjmz",
        "outputId": "8ef0710a-843d-403f-9b86-c317f8d39a0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-- Epoch 1\n",
            "Norm: 10.79, NNZs: 5, Bias: -0.262524, T: 8, Avg. loss: 834.972852\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 37.62, NNZs: 5, Bias: -2.376676, T: 16, Avg. loss: 137353.628962\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 73.06, NNZs: 5, Bias: 1.343910, T: 24, Avg. loss: 111330.564586\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 113.08, NNZs: 5, Bias: -13.102075, T: 32, Avg. loss: 689723.565673\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 142.98, NNZs: 5, Bias: -9.028450, T: 40, Avg. loss: 413714.561073\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 123.49, NNZs: 5, Bias: 5.224480, T: 48, Avg. loss: 3330168.981968\n",
            "Total training time: 0.00 seconds.\n",
            "Convergence after 6 epochs took 0.00 seconds\n",
            "-5.684665232572015\n",
            "1.2533747311072527\n",
            "-4474488.589240138\n",
            "838966.7979825259\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cv = CountVectorizer()\n",
        "cv_fit = cv.fit_transform(corpus)\n",
        "print(\"인덱스 부여 결과 : \", cv.vocabulary_)\n",
        "print(\"인코딩 결과 : \", cv_fit.toarray())"
      ],
      "metadata": {
        "id": "0FLA13PmD3O8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c70f8bf2-fbaf-4af2-b676-a45899d34d82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "인덱스 부여 결과 :  {'제품을': 21, '쓰고': 14, '있어요': 17, '제품의': 22, '디자인이': 5, '우수해요': 16, '성능이': 11, '제품에': 20, '손상이': 12, '제품': 19, '가격이': 1, '비싸요': 8, '정말': 18, '필요한': 26, '제품입니다': 23, '비싸고': 7, '서비스도': 10, '엉망이예요': 15, '가격대비': 0, '좋아요': 24, '필요없는': 25, '기능이': 2, '많은듯': 6, '해요': 27, '서비스': 9, '대응이': 4, '나빠요': 3, '심해요': 13}\n",
            "인코딩 결과 :  [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 1 0 0 0 1 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0]\n",
            " [0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
            " [0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
            " [0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tv = TfidfVectorizer()\n",
        "tv_fit = tv.fit_transform(corpus)\n",
        "\n",
        "print(\"인덱스 부여 결과 : \", tv.vocabulary_)\n",
        "print(\"인코딩 결과 : \", tv_fit.toarray())"
      ],
      "metadata": {
        "id": "ymJWmS-G1GKg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5be7ebaf-b4f6-404d-f656-4158090a0f8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "인덱스 부여 결과 :  {'제품을': 21, '쓰고': 14, '있어요': 17, '제품의': 22, '디자인이': 5, '우수해요': 16, '성능이': 11, '제품에': 20, '손상이': 12, '제품': 19, '가격이': 1, '비싸요': 8, '정말': 18, '필요한': 26, '제품입니다': 23, '비싸고': 7, '서비스도': 10, '엉망이예요': 15, '가격대비': 0, '좋아요': 24, '필요없는': 25, '기능이': 2, '많은듯': 6, '해요': 27, '서비스': 9, '대응이': 4, '나빠요': 3, '심해요': 13}\n",
            "인코딩 결과 :  [[0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.83496155 0.         0.         0.35853734\n",
            "  0.         0.         0.         0.41748077 0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.56467934\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.49881319 0.\n",
            "  0.         0.         0.         0.         0.65751246 0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.74946458\n",
            "  0.         0.         0.         0.         0.66204444 0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.57735027 0.         0.         0.         0.         0.57735027\n",
            "  0.         0.         0.57735027 0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.51905848 0.         0.         0.         0.\n",
            "  0.         0.         0.60439155 0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.60439155 0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.57735027 0.         0.         0.         0.         0.57735027\n",
            "  0.         0.         0.57735027 0.        ]\n",
            " [0.         0.444226   0.         0.         0.         0.\n",
            "  0.         0.51725663 0.         0.         0.51725663 0.\n",
            "  0.         0.         0.         0.51725663 0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.65751246 0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.56467934\n",
            "  0.         0.         0.         0.         0.49881319 0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.65152087\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.75863071 0.         0.         0.        ]\n",
            " [0.         0.         0.5        0.         0.         0.\n",
            "  0.5        0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.5        0.         0.5       ]\n",
            " [0.         0.         0.         0.57735027 0.57735027 0.\n",
            "  0.         0.         0.         0.57735027 0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.54588384 0.63562699 0.         0.         0.         0.\n",
            "  0.         0.         0.54588384 0.         0.         0.\n",
            "  0.         0.         0.         0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 학습"
      ],
      "metadata": {
        "id": "03weijUrtroi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "model = LogisticRegression(C = 0.01) # C : 과적합 방지\n",
        "model.fit(rs, labels)\n",
        "model.score(rs, labels)"
      ],
      "metadata": {
        "id": "68yra3dl1GNC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47dce549-e527-44e9-8d44-86a0a4963eb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.75"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 예측"
      ],
      "metadata": {
        "id": "spl966DDvY5q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_new1 = [\"쓰다가 하루만에 버렸어요\"]\n",
        "X_new2 = [\"저번에 써봤는데 또 샀어요\"]\n",
        "\n",
        "# 토큰화/인코딩/padding\n",
        "\n",
        "en_new1 = tokenizer.texts_to_sequences(X_new1)\n",
        "en_new2 = tokenizer.texts_to_sequences(X_new2)\n",
        "\n",
        "pad_new1 = pad_sequences(en_new1, maxlen = 10)\n",
        "pad_new2 = pad_sequences(en_new2, maxlen = 10)\n",
        "\n",
        "# 예측\n",
        "\n",
        "print(model.score(rs, labels))\n",
        "print(model.predict(pad_new1))\n",
        "print(model.predict(pad_new2))"
      ],
      "metadata": {
        "id": "6EZDv0zhz92A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60ce7643-41fe-4191-d5cf-1f157fa2c8aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.75\n",
            "[1]\n",
            "[1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- CountVectorizer()"
      ],
      "metadata": {
        "id": "EzV7bYtmxcv5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cv = CountVectorizer()\n",
        "cv_fit = cv.fit_transform(corpus)\n",
        "model.fit(cv_fit, labels)\n",
        "model.score(cv_fit, labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dtO_g_Mcvdy7",
        "outputId": "d790445e-9c99-4838-d088-f46ba4122be6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_new1 = [\"하루만에 망가졌어요\"]\n",
        "X_new2 = [\"재구매 하고 싶어요\"]\n",
        "\n",
        "# 토큰화/인코딩/padding\n",
        "# fit() : 분석만 수행\n",
        "# fit_transform() : 분석하고 인코딩을 동시에 수행\n",
        "# transform() : fit()이나 fit_transform()으로 이전에 분석된 결과를 기반으로 인코딩만 수행\n",
        "\n",
        "\n",
        "en_new1_1 = cv.transform(X_new1)\n",
        "en_new1 = cv.transform(X_new1).toarray()\n",
        "en_new2 = cv.transform(X_new2).toarray()\n",
        "\n",
        "# 예측\n",
        "\n",
        "print(model.score(cv_fit, labels))\n",
        "print(en_new1)\n",
        "print(model.predict(en_new1_1))\n",
        "print(model.predict(en_new2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5O7st9Ygvd1a",
        "outputId": "79866afd-d50d-4289-f079-757955f0c3a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0\n",
            "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
            "[0]\n",
            "[0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tv = TfidfVectorizer()\n",
        "tv_fit = tv.fit_transform(corpus)\n",
        "model.fit(tv_fit, labels)\n",
        "model.score(tv_fit, labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7sT3SrQ8vd3i",
        "outputId": "e83ab890-a8db-4ef3-e575-e5c63f04a53c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_new1 = [\"다시는 안살꺼예요\"]\n",
        "X_new2 = [\"벌써 세번째 구매예요\"]\n",
        "\n",
        "en_new1 = tv.transform(X_new1).toarray()\n",
        "en_new2 = tv.transform(X_new2).toarray()\n",
        "\n",
        "# 예측\n",
        "\n",
        "print(model.score(tv_fit, labels))\n",
        "print(en_new1)\n",
        "print(model.predict(en_new1))\n",
        "print(model.predict(en_new2))"
      ],
      "metadata": {
        "id": "wgddX0J8z94c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fda9541-3395-41bb-edab-a550fcd3344b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0\n",
            "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0.]]\n",
            "[0]\n",
            "[0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 임베딩 (Embedding)\n",
        "- 인코딩된 토큰에 의미를 부여하는 작업\n",
        "- 단어들간의 유사성이나 선후 관계등을 분석해서 벡터값(크기, 방향)을 계산하는 작업 -> 최종적으로 실수값으로 저장\n",
        "\n",
        "- 토큰 사이의 관계를 반영 -> 토큰을 기하학적 공간에 맵핑(단어간의 의미적 거리와 방향 특성을 분석)\n",
        "\n",
        "<center>  \n",
        "<img src=\"https://arome1004.cafe24.com/images/machine_learning/textmining08.png\" width=50%>\n",
        "</center>"
      ],
      "metadata": {
        "id": "Af0MWwy27st-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 맵핑의 예시\n",
        "\n",
        "<center>  \n",
        "<img src=\"https://arome1004.cafe24.com/images/machine_learning/textmining09.png\" width=50%>\n",
        "</center>"
      ],
      "metadata": {
        "id": "qf_azQZprnJr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 의미적 또는 문법적 정보를 함축\n",
        "\n",
        "<center>  \n",
        "<img src=\"https://arome1004.cafe24.com/images/machine_learning/textmining10.png\" width=30%>\n",
        "</center>\n",
        "\n",
        "<center>  \n",
        "<img src=\"https://arome1004.cafe24.com/images/machine_learning/textmining11.png\" width=50%>\n",
        "</center>"
      ],
      "metadata": {
        "id": "B1zpVUJarrZb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 단어임베딩\n",
        "  - Word2Vec\n",
        "    - 2013년에 제안\n",
        "    - 단어를 벡터로 변환, 단어들의 지역적 정보를 반영\n",
        "    - CBOW와 SkipGram 방식이 있음\n",
        "  - GloVe\n",
        "    - 2014년에 제안\n",
        "    - 단어들의 전역적 정보를 반영\n",
        "  - FastText\n",
        "    - 2016년에 제안\n",
        "    - 내부단어를 고려\n",
        "- 문장임베딩\n",
        "  - Doc2Vec\n",
        "    - 문장을 벡터로 변환 (RAG)"
      ],
      "metadata": {
        "id": "DUH8oRtg9MS7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FastText를 이용한 임베딩"
      ],
      "metadata": {
        "id": "meS-sfaG-Osr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8szH9sk7sy1",
        "outputId": "8b29f6bc-4fbb-4dcc-d7bf-430416fb55f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import FastText\n",
        "from konlpy.tag import Okt\n",
        "\n",
        "okt = Okt()\n",
        "tonken_texts = [okt.morphs(text) for text in corpus]\n",
        "tonken_texts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GumVX7AZ7tDN",
        "outputId": "b6bd8530-62fb-4ff3-a5a4-ecec931ea481"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['제품', '을', '잘', '쓰고', '쓰고', '있어요'],\n",
              " ['제품', '의', '디자인', '이', '우수해요'],\n",
              " ['성능', '이', '우수해요'],\n",
              " ['제품', '에', '손상', '이', '있어요'],\n",
              " ['제품', '가격', '이', '비싸요'],\n",
              " ['정말', '필요한', '제품', '입니다'],\n",
              " ['가격', '이', '비싸고', '서비스', '도', '엉망', '이', '예요'],\n",
              " ['가격', '대비', '성능', '이', '우수해요'],\n",
              " ['디자인', '이', '좋아요'],\n",
              " ['필요없는', '기능', '이', '많은듯', '해', '요'],\n",
              " ['서비스', '대응', '이', '나빠요'],\n",
              " ['제품', '에', '손상', '이', '심해', '요']]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sentences : 토큰 데이터\n",
        "# vector_size : 벡터의 차원 수\n",
        "# window : 참고할 근처의 단어 수\n",
        "# min_count : 빈도수가 1 이하인 단어는 포함하지 않게 설정\n",
        "# workers : 사용할 CPU 스레드 수\n",
        "# sg : 학습 알고리즘(0 : CBOW, 1 : Skip-gram)\n",
        "\n",
        "fasttext_model = FastText(sentences = tonken_texts, vector_size = 100, window = 5, min_count = 1, workers = 4, sg = 1)\n",
        "\n",
        "# 분석된 벡터데이터에서 해당 단어와 유사한 단어의 유사도를 출력\n",
        "\n",
        "fasttext_model.wv.most_similar(\"좋아요\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ssIhY4BY7tL1",
        "outputId": "4e3877bb-26c4-492e-a249-afc89b3d21f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('있어요', 0.15453237295150757),\n",
              " ('비싸고', 0.13719542324543),\n",
              " ('입니다', 0.13637414574623108),\n",
              " ('의', 0.13334721326828003),\n",
              " ('필요한', 0.11915337294340134),\n",
              " ('이', 0.10063231736421585),\n",
              " ('가격', 0.0753081887960434),\n",
              " ('정말', 0.05897729471325874),\n",
              " ('엉망', 0.05094192177057266),\n",
              " ('디자인', 0.028948016464710236)]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center>\n",
        "<img src=\"https://arome1004.cafe24.com/images/machine_learning/word2vec01.png\" width=50%>\n",
        "</center>"
      ],
      "metadata": {
        "id": "l8C8xRZBF7Uf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 토큰에 벡터값을 할당해서 특성데이터 할당"
      ],
      "metadata": {
        "id": "Dak2PmhfJj__"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def get_sentence_vector(tokens):\n",
        "\n",
        "  vectors = [fasttext_model.wv[word] for word in tokens if word in fasttext_model.wv]\n",
        "  return np.mean(vectors, axis=0) if vectors else np.zeros(fasttext_model.vector_size)\n",
        "\n",
        "X = np.array([get_sentence_vector(tokens) for tokens in tonken_texts])\n",
        "y = np.array(labels)\n",
        "\n",
        "print(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qYvVvS5r7tUN",
        "outputId": "05f4f68b-76ca-4e03-85c1-2a59043d8336"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-1.3982723e-03  8.7348721e-04  2.2858835e-03 ...  1.2321492e-03\n",
            "   5.3302740e-04  1.4801724e-03]\n",
            " [-1.5448843e-03  2.9485970e-04  2.4470183e-04 ... -1.4771712e-04\n",
            "  -5.7047309e-04  2.3577928e-03]\n",
            " [ 9.8547898e-04 -1.2829767e-04  1.0858176e-05 ... -3.0596994e-03\n",
            "   7.7302690e-04  1.8243687e-03]\n",
            " ...\n",
            " [-1.6903660e-04  1.1650912e-03 -5.6157302e-04 ... -2.1907866e-03\n",
            "  -2.9127390e-04  8.5379666e-04]\n",
            " [-3.1782803e-04 -2.5657583e-03  8.7379827e-04 ... -2.5713323e-03\n",
            "  -2.2042534e-04  1.2613480e-03]\n",
            " [ 4.1331103e-04  1.2886156e-03  4.0561883e-04 ... -1.9073678e-03\n",
            "  -6.8082707e-05  9.3037618e-04]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "model = LogisticRegression(C = 0.01)\n",
        "model.fit(X, y)\n",
        "model.score(X, y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rp5DOIipJJ-f",
        "outputId": "cc9de6b2-2298-4f0c-e681-0900c05b909e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9166666666666666"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "okt = Okt()\n",
        "X_new1 = [\"제품이 너무 나빠요\"]\n",
        "X_new2 = [\"제품이 너무 좋아요\"]\n",
        "\n",
        "t1 = okt.morphs(X_new1[0])\n",
        "t2 = okt.morphs(X_new2[0])\n",
        "\n",
        "en_new1 = get_sentence_vector(t1)\n",
        "en_new2 = get_sentence_vector(t2)\n",
        "\n",
        "pre1 = model.predict([en_new1])\n",
        "pre2 = model.predict([en_new2])\n",
        "\n",
        "print(pre1)\n",
        "print(pre2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JaT_pO_OJKA_",
        "outputId": "1874ce92-97a5-4c30-9c64-750678c0532a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0]\n",
            "[1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X)\n",
        "print(model.score(X, y))\n",
        "print(pre1)\n",
        "print(pre2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBRiJ8ZJJKE3",
        "outputId": "a9e47ccb-03ad-450e-a9ca-cc353e60d677"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-1.3982723e-03  8.7348721e-04  2.2858835e-03 ...  1.2321492e-03\n",
            "   5.3302740e-04  1.4801724e-03]\n",
            " [-1.5448843e-03  2.9485970e-04  2.4470183e-04 ... -1.4771712e-04\n",
            "  -5.7047309e-04  2.3577928e-03]\n",
            " [ 9.8547898e-04 -1.2829767e-04  1.0858176e-05 ... -3.0596994e-03\n",
            "   7.7302690e-04  1.8243687e-03]\n",
            " ...\n",
            " [-1.6903660e-04  1.1650912e-03 -5.6157302e-04 ... -2.1907866e-03\n",
            "  -2.9127390e-04  8.5379666e-04]\n",
            " [-3.1782803e-04 -2.5657583e-03  8.7379827e-04 ... -2.5713323e-03\n",
            "  -2.2042534e-04  1.2613480e-03]\n",
            " [ 4.1331103e-04  1.2886156e-03  4.0561883e-04 ... -1.9073678e-03\n",
            "  -6.8082707e-05  9.3037618e-04]]\n",
            "0.9166666666666666\n",
            "[0]\n",
            "[1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kDUDgoj67tdF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NDydqkWE7tgl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Iq8p71EZ7tj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FoxkkmENz9-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ltoyQIDc9lxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vdmh7t_49l4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "grCEDkYs9mMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z78EYfA69mPA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}